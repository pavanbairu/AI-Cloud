{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccfda540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-07-28T13:05:15+00:00', 'source': 'Y:\\\\GitHub\\\\AI Cloud\\\\RAG\\\\data\\\\Pavan_Bairu_Resume.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='Pavan Bairu \\n+918886261654           pavan.bairu1@gmail.com         Warangal, India-506003 \\n \\nProfessional Summary \\nSoftware Developer with 4.6 years of experience in SQL-based backend development for banking and insurance \\ndomains. Skilled in writing and optimizing SQL queries, and building APIs using Python and FastAPI. Also upskilled \\nand hands-on on AI/ML model development and deployment.\\n \\nTechnical Skills \\n• Programming: Python \\n• Databases: SQL, DB2 \\n• Cloud: AWS (S3, EC2, ECS, ECR) \\n• MLOps & Deployment: Docker, CI/CD Pipelines, Flask, FastAPI, GitHub Actions \\n• Data Visualization: Matplotlib, Seaborn \\n \\nWork Experience \\nSystem Engineer | TCS | Feb 2023 – Present \\nIndustry: Insurance, Pensions & Investment | ACD Project (LBG Client) \\n• Tech Stack: Python, SQL(MySQL), FastAPI, SQLAlchemy. \\n• Developed a backend system for automating mutual fund switch requests for retail and institutional \\nclients. \\n• Designed relational database schema for policies, funds, prices, and fund assets using MySQL. \\n• Wrote SQL queries for calculating units, adjusting fund balances, and updating policy fund holdings. \\n• Exposed business logic through FastAPI endpoints and ensured secure database access with \\nSQLAlchemy. \\n• Improved operational efficiency by reducing manual effort and ensuring accurate fund transfers. \\n \\nAssociate Consultant | Capgemini | Jan 2021 – Jan 2023 \\nIndustry: Banking (Cards) | Customer Data Store Project (CDS) (Discover Client) \\n• Tech Stack: SQL (DB2) \\n• Developed and optimized SQL queries to extract, update, and validate customer and transaction data \\nin DB2. \\n• Improved existing query performance by refining joins and conditions based on business logic. \\n• Collaborated with teams to ensure consistent and accurate data across banking systems. \\n• Identified and resolved SQL-related issues in backend processes, enhancing data accuracy. \\n• Performed manual data checks and validations to support business reporting and processing.'),\n",
       " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-07-28T13:05:15+00:00', 'source': 'Y:\\\\GitHub\\\\AI Cloud\\\\RAG\\\\data\\\\Pavan_Bairu_Resume.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}, page_content='Personal Projects \\nCredit Card Transactions Fraud Detection \\n• Problem Statement: Detecting fraud in highly imbalanced credit card transactions (99% legit, 1% \\nfraud) while minimizing false negatives. \\n• Tech Stack: Python, FastAPI, Scikit-learn, GitHub Actions, Docker, AWS EC2, AWS S3. \\n• Addressed imbalanced data (99% non-fraud, 1% fraud) using upsampling and handled missing \\nvalues, outliers, and data drift. \\n• Extracted key features for improved model interpretability. and trained classification models, \\nachieving 90% accuracy in fraud detection. \\n• Deployed with FastAPI, Docker on AWS EC2, and automated CI/CD using GitHub Actions. \\nSQL-LLM-Chatbot \\n• Tech Stack: Python, Streamlit, Euriai LLM, SQLAlchemy, Docker, NeonDB, GitHub Actions, \\nAWS ECS/ECR \\n• Problem Statement: Non-technical users struggle to write SQL queries to retrieve database insights. \\n• Built an LLM-powered assistant that converts natural language to SQL and executes it live. \\n• Developed a Streamlit app that generates SQL using Euriai LLM and runs it on Neon PostgreSQL. \\n• Dynamically injected database schema into LLM prompt using SQLAlchemy. \\n• Containerized app with Docker and managed environment secrets securely. \\n• Automated CI/CD deployment to AWS ECS using GitHub Actions and Amazon ECR. \\n \\nEducation \\nB.Tech in Electronics & Communications Engineering \\nGuru Nanak Institutions Technical campus Hyderabad – 2019. \\n \\nCertifications \\n• AWS Cloud Practitioner CLF – C02 \\n \\nAchievements & Awards \\n• RISING STAR certificate in recognition of good work done from Capgemini. \\n• Certificate of Recognition for outstanding contribution to the TCS-LBG Journey')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(\"Y:\\GitHub\\AI Cloud\\RAG\\data\\Pavan_Bairu_Resume.pdf\")\n",
    "\n",
    "documents = loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9eed60ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-07-28T13:05:15+00:00', 'source': 'Y:\\\\GitHub\\\\AI Cloud\\\\RAG\\\\data\\\\Pavan_Bairu_Resume.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='Pavan Bairu \\n+918886261654           pavan.bairu1@gmail.com         Warangal, India-506003 \\n \\nProfessional Summary \\nSoftware Developer with 4.6 years of experience in SQL-based backend development for banking and insurance'),\n",
       " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-07-28T13:05:15+00:00', 'source': 'Y:\\\\GitHub\\\\AI Cloud\\\\RAG\\\\data\\\\Pavan_Bairu_Resume.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='domains. Skilled in writing and optimizing SQL queries, and building APIs using Python and FastAPI. Also upskilled \\nand hands-on on AI/ML model development and deployment.\\n \\nTechnical Skills \\n• Programming: Python \\n• Databases: SQL, DB2 \\n• Cloud: AWS (S3, EC2, ECS, ECR)'),\n",
       " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-07-28T13:05:15+00:00', 'source': 'Y:\\\\GitHub\\\\AI Cloud\\\\RAG\\\\data\\\\Pavan_Bairu_Resume.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='• Cloud: AWS (S3, EC2, ECS, ECR) \\n• MLOps & Deployment: Docker, CI/CD Pipelines, Flask, FastAPI, GitHub Actions \\n• Data Visualization: Matplotlib, Seaborn \\n \\nWork Experience \\nSystem Engineer | TCS | Feb 2023 – Present \\nIndustry: Insurance, Pensions & Investment | ACD Project (LBG Client)'),\n",
       " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-07-28T13:05:15+00:00', 'source': 'Y:\\\\GitHub\\\\AI Cloud\\\\RAG\\\\data\\\\Pavan_Bairu_Resume.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='• Tech Stack: Python, SQL(MySQL), FastAPI, SQLAlchemy. \\n• Developed a backend system for automating mutual fund switch requests for retail and institutional \\nclients. \\n• Designed relational database schema for policies, funds, prices, and fund assets using MySQL.'),\n",
       " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-07-28T13:05:15+00:00', 'source': 'Y:\\\\GitHub\\\\AI Cloud\\\\RAG\\\\data\\\\Pavan_Bairu_Resume.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='• Wrote SQL queries for calculating units, adjusting fund balances, and updating policy fund holdings. \\n• Exposed business logic through FastAPI endpoints and ensured secure database access with \\nSQLAlchemy.'),\n",
       " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-07-28T13:05:15+00:00', 'source': 'Y:\\\\GitHub\\\\AI Cloud\\\\RAG\\\\data\\\\Pavan_Bairu_Resume.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='SQLAlchemy. \\n• Improved operational efficiency by reducing manual effort and ensuring accurate fund transfers. \\n \\nAssociate Consultant | Capgemini | Jan 2021 – Jan 2023 \\nIndustry: Banking (Cards) | Customer Data Store Project (CDS) (Discover Client) \\n• Tech Stack: SQL (DB2)'),\n",
       " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-07-28T13:05:15+00:00', 'source': 'Y:\\\\GitHub\\\\AI Cloud\\\\RAG\\\\data\\\\Pavan_Bairu_Resume.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='• Tech Stack: SQL (DB2) \\n• Developed and optimized SQL queries to extract, update, and validate customer and transaction data \\nin DB2. \\n• Improved existing query performance by refining joins and conditions based on business logic.'),\n",
       " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-07-28T13:05:15+00:00', 'source': 'Y:\\\\GitHub\\\\AI Cloud\\\\RAG\\\\data\\\\Pavan_Bairu_Resume.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='• Collaborated with teams to ensure consistent and accurate data across banking systems. \\n• Identified and resolved SQL-related issues in backend processes, enhancing data accuracy. \\n• Performed manual data checks and validations to support business reporting and processing.'),\n",
       " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-07-28T13:05:15+00:00', 'source': 'Y:\\\\GitHub\\\\AI Cloud\\\\RAG\\\\data\\\\Pavan_Bairu_Resume.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}, page_content='Personal Projects \\nCredit Card Transactions Fraud Detection \\n• Problem Statement: Detecting fraud in highly imbalanced credit card transactions (99% legit, 1% \\nfraud) while minimizing false negatives. \\n• Tech Stack: Python, FastAPI, Scikit-learn, GitHub Actions, Docker, AWS EC2, AWS S3.'),\n",
       " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-07-28T13:05:15+00:00', 'source': 'Y:\\\\GitHub\\\\AI Cloud\\\\RAG\\\\data\\\\Pavan_Bairu_Resume.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}, page_content='• Addressed imbalanced data (99% non-fraud, 1% fraud) using upsampling and handled missing \\nvalues, outliers, and data drift. \\n• Extracted key features for improved model interpretability. and trained classification models, \\nachieving 90% accuracy in fraud detection.'),\n",
       " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-07-28T13:05:15+00:00', 'source': 'Y:\\\\GitHub\\\\AI Cloud\\\\RAG\\\\data\\\\Pavan_Bairu_Resume.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}, page_content='achieving 90% accuracy in fraud detection. \\n• Deployed with FastAPI, Docker on AWS EC2, and automated CI/CD using GitHub Actions. \\nSQL-LLM-Chatbot \\n• Tech Stack: Python, Streamlit, Euriai LLM, SQLAlchemy, Docker, NeonDB, GitHub Actions, \\nAWS ECS/ECR'),\n",
       " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-07-28T13:05:15+00:00', 'source': 'Y:\\\\GitHub\\\\AI Cloud\\\\RAG\\\\data\\\\Pavan_Bairu_Resume.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}, page_content='AWS ECS/ECR \\n• Problem Statement: Non-technical users struggle to write SQL queries to retrieve database insights. \\n• Built an LLM-powered assistant that converts natural language to SQL and executes it live.'),\n",
       " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-07-28T13:05:15+00:00', 'source': 'Y:\\\\GitHub\\\\AI Cloud\\\\RAG\\\\data\\\\Pavan_Bairu_Resume.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}, page_content='• Developed a Streamlit app that generates SQL using Euriai LLM and runs it on Neon PostgreSQL. \\n• Dynamically injected database schema into LLM prompt using SQLAlchemy. \\n• Containerized app with Docker and managed environment secrets securely.'),\n",
       " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-07-28T13:05:15+00:00', 'source': 'Y:\\\\GitHub\\\\AI Cloud\\\\RAG\\\\data\\\\Pavan_Bairu_Resume.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}, page_content='• Automated CI/CD deployment to AWS ECS using GitHub Actions and Amazon ECR. \\n \\nEducation \\nB.Tech in Electronics & Communications Engineering \\nGuru Nanak Institutions Technical campus Hyderabad – 2019. \\n \\nCertifications \\n• AWS Cloud Practitioner CLF – C02 \\n \\nAchievements & Awards'),\n",
       " Document(metadata={'producer': 'Pdftools SDK', 'creator': 'PyPDF', 'creationdate': '', 'moddate': '2025-07-28T13:05:15+00:00', 'source': 'Y:\\\\GitHub\\\\AI Cloud\\\\RAG\\\\data\\\\Pavan_Bairu_Resume.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}, page_content='Achievements & Awards \\n• RISING STAR certificate in recognition of good work done from Capgemini. \\n• Certificate of Recognition for outstanding contribution to the TCS-LBG Journey')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_spiltter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = text_spiltter.split_documents(documents)\n",
    "\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "514dacc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.2:1b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d93a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "094ffcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vector_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5fa4967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model=\"llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "10c106b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a highly intelligent resume parsing assistant.\n",
    "\n",
    "Given the resume content below:\n",
    "{context}\n",
    "\n",
    "Your task is to:\n",
    "- Carefully read the question and extract only the **relevant** information.\n",
    "- Focus on **specific sections** mentioned in the question (e.g., TCS, Capgemini, Projects).\n",
    "- If the question is **multi-part**, extract and structure all parts clearly.\n",
    "- Always respond in a **valid Python dictionary** format.\n",
    "- Use **nested dictionaries or lists** if necessary.\n",
    "- Do not include unrelated information, even if found elsewhere in the resume.\n",
    "- If the information is not found, return an empty dictionary: {{}}\n",
    "\n",
    "Now answer the following question:\n",
    "\"{question}\"\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "215a9d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt_template}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1c012de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Skills\n",
      "```\n",
      "{\n",
      "    \"Technical\": [\n",
      "        \"Data Analysis\",\n",
      "        \"Machine Learning\",\n",
      "        \"Python\",\n",
      "        \"R\",\n",
      "        \"SQL\"\n",
      "    ],\n",
      "    \"Programming Languages\": [\n",
      "        \"Python 3.8\",\n",
      "        \"NumPy\"\n",
      "    ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "question = \"what are all the skills does pavan have?\"\n",
    "result = retrieval_qa({\"query\": question})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41039a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"background\": \"\",\n",
      "    \"education\": {},\n",
      "    \"experience\": {\n",
      "        \"TCS\": {\n",
      "            \"system_engineer\": [\n",
      "                {\n",
      "                    \"backend_system\": \"AWS\",\n",
      "                    \"cloud\": \"S3, EC2, ECS, ECR\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        \"personal_projects\": {\n",
      "            \"Credit Card Transactions Fraud Detection\": {\n",
      "                \"tech_stack\": [\n",
      "                    \"Python\", \n",
      "                    \"FastAPI\", \n",
      "                    \"Scikit-learn\", \n",
      "                    \"GitHub Actions\", \n",
      "                    \"Docker\", \n",
      "                    \"AWS EC2\", \n",
      "                    \"AWS S3\"\n",
      "                ],\n",
      "                \"problem_statement\": \"Detecting fraud in highly imbalanced credit card transactions (99% legit, 1% fraud) while minimizing false negatives.\",\n",
      "                \"tech_stack_used\": []\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = retrieval_qa({\"query\": \"What backend system did the candidate build at TCS and what technologies were used?\"})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b5d48978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Company\": {\n",
      "        \"Name\": \"Capgemini\",\n",
      "        \"Industry\": \"Software Engineering\"\n",
      "    },\n",
      "    \"Job Title\": \"System Engineer\",\n",
      "    \"Time Period\": [\n",
      "        \"2023 - Present\"\n",
      "    ],\n",
      "    \"Experience\": [\n",
      "        {\n",
      "            \"Company\": \"TCS (System Engineer)\",\n",
      "            \"Date\": \"Feb 2023 - Present\",\n",
      "            \"Industry\": \"Insurance, Pensions & Investment\",\n",
      "            \"Client\": \"ACD Project (LBG Client)\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = retrieval_qa({\"query\": \"Summarize the responsibilities held at Capgemini?\"})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "39352809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Project Goals\": [\n",
      "        \"Develop a conversational AI chatbot that converts natural language inputs to SQL queries\",\n",
      "        \"Integrate an LLM-powered assistant with AWS ECS/ECR for database query execution\"\n",
      "    ],\n",
      "    \"Deployment Overview\": [\n",
      "        \"AWS ECS/ECR containerized infrastructure for the chatbot\",\n",
      "        \"GitHub Actions workflow file for continuous integration and deployment\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = retrieval_qa({\"query\": \"What are the main goals of the SQL-LLM-Chatbot project and how was it deployed?\"})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9ca07a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the parsed response:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"Professional Summary\": {\n",
      "        \"Personal Details\": {\n",
      "            \"Name\": \"Pavan Bairu\",\n",
      "            \"Contact Information\": {\n",
      "                \"Email\": \"pavan.bairu1@gmail.com\",\n",
      "                \"Phone Number\": \"+918886261654\"\n",
      "            },\n",
      "            \"Location\": \"Warangal, India - 506003\"\n",
      "        }\n",
      "    },\n",
      "    \"Education\": {\n",
      "        \"Academic History\": {\n",
      "            \"Degree\": \"B.Tech in Electronics & Communications Engineering\",\n",
      "            \"Campus\": \"Guru Nanak Institutions Technical Campus\",\n",
      "            \"Institution\": \"Hyderabad\",\n",
      "            \"Year of Graduation\": 2019\n",
      "        }\n",
      "    },\n",
      "    \"Certifications\": {},\n",
      "    \"Achievements and Awards\": {\n",
      "        \"Personal Achievements\": {\n",
      "            \"RISING STAR Certificate from Capgemini\"\n",
      "        },\n",
      "        \"Professional Certifications\": {\n",
      "            \"AWS Cloud Practitioner CLF - C02\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "Note: The 'TCS' section was not explicitly mentioned, but based on the context, it can be inferred that TCS is likely the company. Therefore, I have added a \"Personal Details\" section under the \"Professional Summary\" category.\n",
      "\n",
      "However, to correct my previous response and follow the format as requested, here's another attempt at extracting only the relevant information:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"Professional Summary\": {\n",
      "        \"Personal Details\": {\n",
      "            \"Name\": \"Pavan Bairu\",\n",
      "            \"Contact Information\": {\n",
      "                \"Email\": \"pavan.bairu1@gmail.com\"\n",
      "            }\n",
      "        },\n",
      "        \"Education\": {\n",
      "            \"Academic History\": {\n",
      "                \"Degree\": \"B.Tech in Electronics & Communications Engineering\",\n",
      "                \"Campus\": \"Guru Nanak Institutions Technical Campus\",\n",
      "                \"Institution\": \"Hyderabad\"\n",
      "            }\n",
      "        },\n",
      "        \"Certifications\": {},\n",
      "        \"Achievements and Awards\": {\n",
      "            \"Personal Achievements\": {\n",
      "                \"RISING STAR Certificate from Capgemini\"\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "result = retrieval_qa({\"query\": \"fetch the personal details of the candidate\"})\n",
    "print(result['result'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-cloud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
